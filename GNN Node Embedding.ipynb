{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43471dc8",
   "metadata": {},
   "source": [
    "# GNN Node Embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d133983",
   "metadata": {},
   "source": [
    "Firstly, we import the necessary libraries to the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c8f4151",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "pd.set_option('display.max_columns', None)\n",
    "from torch_geometric.nn import SAGEConv #, Sequential\n",
    "from torch.nn import ReLU, Sequential, Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3e6af6",
   "metadata": {},
   "source": [
    "First lets set the seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33accef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1337) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc35cbe6",
   "metadata": {},
   "source": [
    "Then we read the csv file to the notebook and look at the top 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f4c835b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Loading the data - Adapt the file location according to your folder structure\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m3\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "# Loading the data - Adapt the file location according to your folder structure\n",
    "df = pd.read_csv(\"\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d179cd2",
   "metadata": {},
   "source": [
    "I will create a copy of the original dataframe so I can remap the embeddings to the original items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06392eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a copy of the original DataFrame\n",
    "df_original = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648cce5",
   "metadata": {},
   "source": [
    "To create nodes for the location. I decided to create a location variable which is consistent of \"il\", \"ilce\": and \"mahalle\". Since we had a lot of Nan values in these column and dealing with such uncertain values are combersome and we having mand data points, I decided to remove the data points with nans in this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50ca5292",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['il', 'ilce', 'mahalle'])\n",
    "df['location'] = df['il'] + '-' + df['ilce'] + '-' + df['mahalle']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f5f493",
   "metadata": {},
   "source": [
    "Then I set up the node mappings here. I want to create a unique node for each item. Since we have duplicates in the tabular data. I will deal with them here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78176e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "location_mapping = {loc: i for i, loc in enumerate(df['location'].unique())}\n",
    "df['location_id'] = df['location'].map(location_mapping)\n",
    "\n",
    "client_id_mapping = {id: i for i, id in enumerate(df['client_id'].unique())}\n",
    "df['client_id'] = df['client_id'].map(client_id_mapping)\n",
    "\n",
    "product_id_mapping = {id: i for i, id in enumerate(df['product_id'].unique())}\n",
    "df['product_id'] = df['product_id'].map(product_id_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5b359af",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_client_one_hot = pd.get_dummies(df[[\"client_id\", \"event_type\", \"transaction_type\", \"device_category\"]], columns=[\"device_category\", \"event_type\", \"transaction_type\"])\n",
    "nodes_client_one_hot = nodes_client_one_hot.groupby(\"client_id\").mean().reset_index()\n",
    "\n",
    "nodes_product_one_hot = pd.get_dummies(df[[\"product_id\", \"product_list_position\", \"kategori_1\", \"kategori_2\", \"kategori_3\"]], columns=[\"product_list_position\", \"kategori_1\", \"kategori_2\", \"kategori_3\"])\n",
    "nodes_product_one_hot = nodes_product_one_hot.groupby(\"product_id\").mean().reset_index()\n",
    "\n",
    "nodes_location = pd.DataFrame({'location_id': list(location_mapping.values())})\n",
    "nodes_location['dummy'] = 1  # Since location has no feature and we need to use its id as the data index,\n",
    "# I create a dummy variable here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9f7ca",
   "metadata": {},
   "source": [
    "I decide to create a simple graph where we have the following graph structure:\n",
    "\n",
    "            location --> product <--> client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70b3e250",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TR\\AppData\\Local\\Temp\\ipykernel_14648\\2260469951.py:1: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  client_product_edges = torch.tensor([df['client_id'].values, df['product_id'].values], dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "client_product_edges = torch.tensor([df['client_id'].values, df['product_id'].values], dtype=torch.long)\n",
    "product_client_edges = torch.tensor([df['product_id'].values, df['client_id'].values], dtype=torch.long)\n",
    "product_location_edges = torch.tensor([df['product_id'].values, df['location_id'].values], dtype=torch.long)\n",
    "location_product_edges = torch.tensor([df['location_id'].values, df['product_id'].values], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789bb6c0",
   "metadata": {},
   "source": [
    "The graph needs a specific data structure called HeteroData. This is needed because our aim is to create embedding vectors with different types of nodes. So we store our edges and nodes in this special data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b478f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = HeteroData()\n",
    "data['client'].x = torch.tensor(nodes_client_one_hot.values, dtype=torch.float)\n",
    "data['product'].x = torch.tensor(nodes_product_one_hot.values, dtype=torch.float)\n",
    "data['location'].x = torch.tensor(nodes_location.values, dtype=torch.float)\n",
    "data['client', 'interacts', 'product'].edge_index = client_product_edges\n",
    "data['product', 'interacted_by', 'client'].edge_index = product_client_edges\n",
    "data['product', 'be_found_in', 'location'].edge_index = product_location_edges\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61ac44c",
   "metadata": {},
   "source": [
    "We then define the Graphical Neural network model. We used a really simple graph model where we have one SAGEConv layer and one Linear layer where we project the dimensions of the embeddings to be 2 dimensional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55c29dcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class CustomHeteroGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, uniform_out_channels):\n",
    "        super(CustomHeteroGNN, self).__init__()\n",
    "        # Defining the Layers\n",
    "        self.conv1_client_product = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv1_product_client = SAGEConv((-1, -1), hidden_channels)\n",
    "        self.conv1_product_location = SAGEConv((-1, -1), hidden_channels)\n",
    "\n",
    "        # PRoject Layers to be 2 dimensional\n",
    "        self.client_out = Linear(hidden_channels, uniform_out_channels)\n",
    "        self.product_out = Linear(hidden_channels, uniform_out_channels)\n",
    "        self.location_out = Linear(hidden_channels, uniform_out_channels)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Client => Product\n",
    "        x_client_to_product = F.relu(self.conv1_client_product((x_dict['client'], x_dict['product']), edge_index_dict[('client', 'interacts', 'product')]))\n",
    "        # Product => Client \n",
    "        x_product_to_client = F.relu(self.conv1_product_client((x_dict['product'], x_dict['client']), edge_index_dict[('product', 'interacted_by', 'client')]))\n",
    "        # Product => Location\n",
    "        x_product_to_location = F.relu(self.conv1_product_location((x_dict['product'], x_dict['location']), edge_index_dict[('product', 'be_found_in', 'location')]))\n",
    "\n",
    "        # Fill our output dictionary so we can access them outside of the function\n",
    "        x_dict['client'] = self.client_out(x_client_to_product)\n",
    "        x_dict['product'] = self.product_out(x_product_to_client)\n",
    "        x_dict['location'] = self.location_out(x_product_to_location)\n",
    "\n",
    "        return x_dict\n",
    "\n",
    "\n",
    "\n",
    "def train(model, data, optimizer, epochs=120):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        z_dict = model(data.x_dict, data.edge_index_dict)\n",
    "        \n",
    "        # Simple mse loss that takes the difference between the prediction and 0 tensor and averages it\n",
    "        \n",
    "        loss = sum(F.mse_loss(z, torch.zeros_like(z)) for z in z_dict.values()) / len(z_dict.values())\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f'Epoch {epoch}: Loss {loss.item()}')\n",
    "    print(f'Epoch {epoch}: Loss {loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed9408",
   "metadata": {},
   "source": [
    "Now lets train our model and observe the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "103c2dfd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 862893.0\n",
      "Epoch 10: Loss 189673.640625\n",
      "Epoch 20: Loss 26005.349609375\n",
      "Epoch 30: Loss 5563.419921875\n",
      "Epoch 40: Loss 6763.08447265625\n",
      "Epoch 50: Loss 2459.5380859375\n",
      "Epoch 60: Loss 817.9625854492188\n",
      "Epoch 70: Loss 306.91796875\n",
      "Epoch 80: Loss 219.52471923828125\n",
      "Epoch 90: Loss 172.0084228515625\n",
      "Epoch 100: Loss 128.01121520996094\n",
      "Epoch 110: Loss 99.48551177978516\n",
      "Epoch 119: Loss 86.44808197021484\n"
     ]
    }
   ],
   "source": [
    "model = CustomHeteroGNN(hidden_channels=64, uniform_out_channels=2)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train(model, data, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbb6f4",
   "metadata": {},
   "source": [
    "We see that the loss in decreaseing. That being said this is a fairly simple loss function and if given time a better loss function should be used.\n",
    "\n",
    "Now lets look at our embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eeecab9d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client embeddings shape: torch.Size([50778, 2])\n",
      "Product embeddings shape: torch.Size([56193, 2])\n",
      "Location embeddings shape: torch.Size([10292, 2])\n"
     ]
    }
   ],
   "source": [
    "model.eval()  \n",
    "\n",
    "with torch.no_grad():  # Disables gradient computation so we dont change traıned model\n",
    "    z_dict = model(data.x_dict, data.edge_index_dict)\n",
    "\n",
    "client_embeddings = z_dict['client']  \n",
    "product_embeddings = z_dict['product']  \n",
    "location_embeddings = z_dict['location']  \n",
    "\n",
    "print(\"Client embeddings shape:\", client_embeddings.shape)\n",
    "print(\"Product embeddings shape:\", product_embeddings.shape)\n",
    "print(\"Location embeddings shape:\", location_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff971ae",
   "metadata": {},
   "source": [
    "We get 2 dimensional embeddings for each unique item type. It is possible to get a higher dimensional embeddings but will not further invertigate this for the simplicity of the case.\n",
    "\n",
    "We can evaluate our embeddings on a local level and a global level.\n",
    "\n",
    "To evaluate our embeddings on the local level, we can simply pick two embeddings and look at their original form and compare it to their cosine similarity. A higher cosine similarity means our embeddings are closer in the 2 dimensional space and the products should reflect that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62f2dbbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Link Prediction Score: -0.5370484590530396\n"
     ]
    }
   ],
   "source": [
    "def predict_link(embedding1, embedding2):\n",
    "    cos_sim = torch.nn.functional.cosine_similarity(embedding1, embedding2, dim=0)\n",
    "    return cos_sim.item()\n",
    "\n",
    "# Ids for example \n",
    "product_one_id = 10\n",
    "product_two_id = 150\n",
    "\n",
    "product1_embedding = product_embeddings[product_one_id]\n",
    "product2_embedding = product_embeddings[product_two_id]\n",
    "similarity_score = predict_link(product1_embedding, product2_embedding)\n",
    "print(f'Link Prediction Score: {similarity_score}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "378b4f3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second id is 11396543\n",
      "Second id is 13205323\n"
     ]
    }
   ],
   "source": [
    "index_to_product_id = {index: product_id for product_id, index in product_id_mapping.items()}\n",
    "\n",
    "product1_index = product_one_id  \n",
    "product2_index = product_two_id  \n",
    "\n",
    "\n",
    "original_product1_id = index_to_product_id[product1_index]\n",
    "original_product2_id = index_to_product_id[product2_index]\n",
    "\n",
    "print(f\"Second id is {original_product1_id}\")\n",
    "print(f\"Second id is {original_product2_id}\")\n",
    "\n",
    "product1_info = df_original[df_original['product_id'] == original_product1_id]\n",
    "product2_info = df_original[df_original['product_id'] == original_product2_id]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e898676b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_list_position</th>\n",
       "      <th>kategori_1</th>\n",
       "      <th>kategori_2</th>\n",
       "      <th>kategori_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1781</th>\n",
       "      <td>11396543</td>\n",
       "      <td>12</td>\n",
       "      <td>kiralik</td>\n",
       "      <td>konut</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27022</th>\n",
       "      <td>11396543</td>\n",
       "      <td>26</td>\n",
       "      <td>kiralik</td>\n",
       "      <td>konut</td>\n",
       "      <td>daire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      product_id  product_list_position kategori_1 kategori_2 kategori_3\n",
       "1781    11396543                     12    kiralik      konut        NaN\n",
       "27022   11396543                     26    kiralik      konut      daire"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product1_info[[\"product_id\", \"product_list_position\", \"kategori_1\", \"kategori_2\", \"kategori_3\"]].head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c20f298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_list_position</th>\n",
       "      <th>kategori_1</th>\n",
       "      <th>kategori_2</th>\n",
       "      <th>kategori_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2158</th>\n",
       "      <td>13205323</td>\n",
       "      <td>2</td>\n",
       "      <td>satilik</td>\n",
       "      <td>konut</td>\n",
       "      <td>daire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145760</th>\n",
       "      <td>13205323</td>\n",
       "      <td>1</td>\n",
       "      <td>satilik</td>\n",
       "      <td>konut</td>\n",
       "      <td>daire</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       product_id  product_list_position kategori_1 kategori_2 kategori_3\n",
       "2158     13205323                      2    satilik      konut      daire\n",
       "145760   13205323                      1    satilik      konut      daire"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product2_info[[\"product_id\", \"product_list_position\", \"kategori_1\", \"kategori_2\", \"kategori_3\"]].head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58bdb7",
   "metadata": {},
   "source": [
    "We can also look at the performance of our embeddings on a global level with KMeans algorithm. K-means algorithm is also an unsupervized learning algorithm that tries to create natural clusters. Then we can use these clusters to see if their is huge inter cluster difference or not by using the silhouette score which is calculated as follow:\n",
    "\n",
    "                                        (b - a) / max(a, b)\n",
    "                                        \n",
    "This will take some time!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "01c308e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette Score: 0.4074034094810486\n",
      "Silhouette Score: 0.40675559639930725\n",
      "Silhouette Score: 0.3888115882873535\n",
      "Silhouette Score: 0.4548274278640747\n",
      "Silhouette Score: 0.4172956347465515\n"
     ]
    }
   ],
   "source": [
    "for i in range(2,7):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42).fit(client_embeddings)\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    silhouette_avg = silhouette_score(client_embeddings, labels)\n",
    "    print(f'Silhouette Score for {i} clusters: {silhouette_avg}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfe3fc1",
   "metadata": {},
   "source": [
    "We see that overall best clustering in our range is with 4 groups of customers. Since the silhouette score is between 1 and -1 where 1 is a perfect clustering. Our clustering seem to be on the Ok side."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
